{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xgOzqXvbVeuI"
   },
   "source": [
    "# Download Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "collapsed": true,
    "executionInfo": {
     "elapsed": 5233,
     "status": "ok",
     "timestamp": 1740516326739,
     "user": {
      "displayName": "Zihao Ye",
      "userId": "17726494799981155241"
     },
     "user_tz": 300
    },
    "id": "V8OoBBs_kG-w",
    "outputId": "8479b0ee-0e8e-43f4-e459-108c8bf4d35e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting git+https://github.com/openai/CLIP.git\n",
      "  Cloning https://github.com/openai/CLIP.git to /tmp/pip-req-build-f50l89ua\n",
      "  Running command git clone --filter=blob:none --quiet https://github.com/openai/CLIP.git /tmp/pip-req-build-f50l89ua\n",
      "  Resolved https://github.com/openai/CLIP.git to commit dcba3cb2e2827b402d2701e7e1c7d9fed8a20ef1\n",
      "  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25hRequirement already satisfied: open_clip_torch in /usr2/zihaoye/miniconda3/envs/time_series/lib/python3.10/site-packages (2.31.0)\n",
      "Requirement already satisfied: ftfy in /usr2/zihaoye/miniconda3/envs/time_series/lib/python3.10/site-packages (from clip==1.0) (6.3.1)\n",
      "Requirement already satisfied: packaging in /usr2/zihaoye/miniconda3/envs/time_series/lib/python3.10/site-packages (from clip==1.0) (24.1)\n",
      "Requirement already satisfied: regex in /usr2/zihaoye/miniconda3/envs/time_series/lib/python3.10/site-packages (from clip==1.0) (2024.11.6)\n",
      "Requirement already satisfied: tqdm in /usr2/zihaoye/miniconda3/envs/time_series/lib/python3.10/site-packages (from clip==1.0) (4.66.5)\n",
      "Requirement already satisfied: torch in /usr2/zihaoye/miniconda3/envs/time_series/lib/python3.10/site-packages (from clip==1.0) (2.4.1)\n",
      "Requirement already satisfied: torchvision in /usr2/zihaoye/miniconda3/envs/time_series/lib/python3.10/site-packages (from clip==1.0) (0.19.1)\n",
      "Requirement already satisfied: huggingface-hub in /usr2/zihaoye/miniconda3/envs/time_series/lib/python3.10/site-packages (from open_clip_torch) (0.29.1)\n",
      "Requirement already satisfied: safetensors in /usr2/zihaoye/miniconda3/envs/time_series/lib/python3.10/site-packages (from open_clip_torch) (0.5.2)\n",
      "Requirement already satisfied: timm in /usr2/zihaoye/miniconda3/envs/time_series/lib/python3.10/site-packages (from open_clip_torch) (1.0.15)\n",
      "Requirement already satisfied: filelock in /usr2/zihaoye/miniconda3/envs/time_series/lib/python3.10/site-packages (from torch->clip==1.0) (3.13.1)\n",
      "Requirement already satisfied: typing-extensions>=4.8.0 in /usr2/zihaoye/miniconda3/envs/time_series/lib/python3.10/site-packages (from torch->clip==1.0) (4.12.2)\n",
      "Requirement already satisfied: sympy in /usr2/zihaoye/miniconda3/envs/time_series/lib/python3.10/site-packages (from torch->clip==1.0) (1.13.3)\n",
      "Requirement already satisfied: networkx in /usr2/zihaoye/miniconda3/envs/time_series/lib/python3.10/site-packages (from torch->clip==1.0) (3.3)\n",
      "Requirement already satisfied: jinja2 in /usr2/zihaoye/miniconda3/envs/time_series/lib/python3.10/site-packages (from torch->clip==1.0) (3.1.4)\n",
      "Requirement already satisfied: fsspec in /usr2/zihaoye/miniconda3/envs/time_series/lib/python3.10/site-packages (from torch->clip==1.0) (2023.10.0)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.1.105 in /usr2/zihaoye/miniconda3/envs/time_series/lib/python3.10/site-packages (from torch->clip==1.0) (12.1.105)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.1.105 in /usr2/zihaoye/miniconda3/envs/time_series/lib/python3.10/site-packages (from torch->clip==1.0) (12.1.105)\n",
      "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.1.105 in /usr2/zihaoye/miniconda3/envs/time_series/lib/python3.10/site-packages (from torch->clip==1.0) (12.1.105)\n",
      "Requirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in /usr2/zihaoye/miniconda3/envs/time_series/lib/python3.10/site-packages (from torch->clip==1.0) (9.1.0.70)\n",
      "Requirement already satisfied: nvidia-cublas-cu12==12.1.3.1 in /usr2/zihaoye/miniconda3/envs/time_series/lib/python3.10/site-packages (from torch->clip==1.0) (12.1.3.1)\n",
      "Requirement already satisfied: nvidia-cufft-cu12==11.0.2.54 in /usr2/zihaoye/miniconda3/envs/time_series/lib/python3.10/site-packages (from torch->clip==1.0) (11.0.2.54)\n",
      "Requirement already satisfied: nvidia-curand-cu12==10.3.2.106 in /usr2/zihaoye/miniconda3/envs/time_series/lib/python3.10/site-packages (from torch->clip==1.0) (10.3.2.106)\n",
      "Requirement already satisfied: nvidia-cusolver-cu12==11.4.5.107 in /usr2/zihaoye/miniconda3/envs/time_series/lib/python3.10/site-packages (from torch->clip==1.0) (11.4.5.107)\n",
      "Requirement already satisfied: nvidia-cusparse-cu12==12.1.0.106 in /usr2/zihaoye/miniconda3/envs/time_series/lib/python3.10/site-packages (from torch->clip==1.0) (12.1.0.106)\n",
      "Requirement already satisfied: nvidia-nccl-cu12==2.20.5 in /usr2/zihaoye/miniconda3/envs/time_series/lib/python3.10/site-packages (from torch->clip==1.0) (2.20.5)\n",
      "Requirement already satisfied: nvidia-nvtx-cu12==12.1.105 in /usr2/zihaoye/miniconda3/envs/time_series/lib/python3.10/site-packages (from torch->clip==1.0) (12.1.105)\n",
      "Requirement already satisfied: triton==3.0.0 in /usr2/zihaoye/miniconda3/envs/time_series/lib/python3.10/site-packages (from torch->clip==1.0) (3.0.0)\n",
      "Requirement already satisfied: nvidia-nvjitlink-cu12 in /usr2/zihaoye/miniconda3/envs/time_series/lib/python3.10/site-packages (from nvidia-cusolver-cu12==11.4.5.107->torch->clip==1.0) (12.6.68)\n",
      "Requirement already satisfied: wcwidth in /usr2/zihaoye/miniconda3/envs/time_series/lib/python3.10/site-packages (from ftfy->clip==1.0) (0.2.13)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /usr2/zihaoye/miniconda3/envs/time_series/lib/python3.10/site-packages (from huggingface-hub->open_clip_torch) (6.0.2)\n",
      "Requirement already satisfied: requests in /usr2/zihaoye/miniconda3/envs/time_series/lib/python3.10/site-packages (from huggingface-hub->open_clip_torch) (2.32.3)\n",
      "Requirement already satisfied: numpy in /usr2/zihaoye/miniconda3/envs/time_series/lib/python3.10/site-packages (from torchvision->clip==1.0) (1.26.0)\n",
      "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr2/zihaoye/miniconda3/envs/time_series/lib/python3.10/site-packages (from torchvision->clip==1.0) (10.4.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /usr2/zihaoye/miniconda3/envs/time_series/lib/python3.10/site-packages (from jinja2->torch->clip==1.0) (2.1.5)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /usr2/zihaoye/miniconda3/envs/time_series/lib/python3.10/site-packages (from requests->huggingface-hub->open_clip_torch) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr2/zihaoye/miniconda3/envs/time_series/lib/python3.10/site-packages (from requests->huggingface-hub->open_clip_torch) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr2/zihaoye/miniconda3/envs/time_series/lib/python3.10/site-packages (from requests->huggingface-hub->open_clip_torch) (2.2.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr2/zihaoye/miniconda3/envs/time_series/lib/python3.10/site-packages (from requests->huggingface-hub->open_clip_torch) (2025.1.31)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr2/zihaoye/miniconda3/envs/time_series/lib/python3.10/site-packages (from sympy->torch->clip==1.0) (1.3.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Requirement already satisfied: git-lfs in /usr2/zihaoye/miniconda3/envs/time_series/lib/python3.10/site-packages (1.6)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "data_dir = 'data'\n",
    "\n",
    "%pip install git+https://github.com/openai/CLIP.git open_clip_torch\n",
    "%pip install git-lfs\n",
    "\n",
    "annotations_folder = os.path.join(data_dir, \"annotations\")\n",
    "if not os.path.exists(annotations_folder):\n",
    "    print(f\"{annotations_folder} folder does not exist, downloading...\")\n",
    "    !wget -P $data_dir http://images.cocodataset.org/annotations/annotations_trainval2017.zip\n",
    "    !unzip $data_dir/annotations_trainval2017.zip -d $data_dir\n",
    "\n",
    "val_folder = os.path.join(data_dir, \"val2017\")\n",
    "if not os.path.exists(val_folder):\n",
    "    print(f\"{val_folder} folder does not exist, downloading...\")\n",
    "    !wget -P $data_dir http://images.cocodataset.org/zips/val2017.zip\n",
    "    !unzip $data_dir/val2017.zip -d $data_dir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "executionInfo": {
     "elapsed": 6,
     "status": "ok",
     "timestamp": 1740516900053,
     "user": {
      "displayName": "Zihao Ye",
      "userId": "17726494799981155241"
     },
     "user_tz": 300
    },
    "id": "QaartCskkOQL"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-02-25 17:41:43.854284: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoConfig, AutoModel, AutoProcessor\n",
    "import open_clip\n",
    "import sys\n",
    "sys.path.append('../..')\n",
    "import torch\n",
    "from torchvision.datasets import CocoCaptions\n",
    "from PIL import Image\n",
    "import clip\n",
    "from tqdm import tqdm\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jBwPTy7Ok9U6"
   },
   "source": [
    "## Encoder Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "executionInfo": {
     "elapsed": 10,
     "status": "ok",
     "timestamp": 1740516902371,
     "user": {
      "displayName": "Zihao Ye",
      "userId": "17726494799981155241"
     },
     "user_tz": 300
    },
    "id": "Yf9X8yGkV5-n"
   },
   "outputs": [],
   "source": [
    "from transformers import AutoConfig, AutoModel\n",
    "import os\n",
    "import shutil\n",
    "from safetensors.torch import load_file\n",
    "\n",
    "def load_triplet_clip_encoders(\n",
    "    repo_name=\"TripletCLIP/CC12M_TripletCLIP_ViTB12\",\n",
    "    text_path='',\n",
    "    image_path='',\n",
    "    work_dir=\"/content\",\n",
    "    device=\"cuda\"\n",
    "):\n",
    "    repo_url = f\"https://huggingface.co/{repo_name}\"\n",
    "\n",
    "    repo_folder_name = repo_name.split(\"/\")[-1]  # e.g. CC12M_TripletCLIP_ViTB12\n",
    "    target_path = os.path.join(work_dir, repo_folder_name)\n",
    "    if os.path.exists(target_path):\n",
    "        shutil.rmtree(target_path)\n",
    "\n",
    "    !git clone $repo_url $target_path\n",
    "    !cd $target_path\n",
    "    !git lfs install\n",
    "\n",
    "    text_config_path = os.path.join(target_path, text_path)\n",
    "    image_config_path = os.path.join(target_path, image_path)\n",
    "\n",
    "    text_config = AutoConfig.from_pretrained(text_config_path, local_files_only=True, trust_remote_code=True, device_map=\"auto\")\n",
    "    image_config = AutoConfig.from_pretrained(image_config_path, local_files_only=True, trust_remote_code=True, device_map=\"auto\")\n",
    "\n",
    "    text_encoder = AutoModel.from_config(text_config, trust_remote_code=True)\n",
    "    image_encoder = AutoModel.from_config(image_config, trust_remote_code=True)\n",
    "\n",
    "    text_encoder.load_state_dict(load_file(os.path.join(text_config_path, 'model.safetensors')), strict=False)\n",
    "    image_encoder.load_state_dict(load_file(os.path.join(image_config_path, 'model.safetensors')), strict=False)\n",
    "\n",
    "    text_encoder.to(device)\n",
    "    image_encoder.to(device)\n",
    "\n",
    "    text_encoder.eval()\n",
    "    image_encoder.eval()\n",
    "\n",
    "    return text_encoder, image_encoder\n",
    "\n",
    "def transform(\n",
    "    dataset,\n",
    "    image_encoder,\n",
    "    text_encoder,\n",
    "    processor,\n",
    "    tokenizer,\n",
    "    device='cuda',\n",
    "    sample_size=None\n",
    "):\n",
    "    from itertools import islice\n",
    "    image_features = []\n",
    "    text_features = []\n",
    "\n",
    "    if sample_size is not None and sample_size <= 5000:\n",
    "        dataset_iter = islice(dataset, sample_size)\n",
    "    else:\n",
    "        dataset_iter = dataset\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for image, captions in tqdm(dataset_iter, total=sample_size):\n",
    "            image_input = processor(images=image, return_tensors=\"pt\").to(device)\n",
    "            image_features.append(image_encoder(image_input))\n",
    "\n",
    "            captions = captions[0:5]\n",
    "            caption_input = tokenizer(captions).to(device)\n",
    "            text_features.extend(text_encoder(caption_input).text_embeds)\n",
    "\n",
    "        image_features = torch.stack(image_features).squeeze()\n",
    "        image_features /= image_features.norm(dim=-1, keepdim=True)\n",
    "\n",
    "        print(image_features.shape)\n",
    "        text_features = torch.stack(text_features)\n",
    "        text_features /= text_features.norm(dim=-1, keepdim=True)\n",
    "\n",
    "    return image_features, text_features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QzOGTGjBgTbt"
   },
   "source": [
    "# Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 39,
     "status": "ok",
     "timestamp": 1740516364266,
     "user": {
      "displayName": "Zihao Ye",
      "userId": "17726494799981155241"
     },
     "user_tz": 300
    },
    "id": "Rml1nq81gU8q",
    "outputId": "6d45efd9-0bd2-4449-9214-6d79fd421b7d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading annotations into memory...\n",
      "Done (t=0.03s)\n",
      "creating index...\n",
      "index created!\n"
     ]
    }
   ],
   "source": [
    "coco = CocoCaptions(root=val_folder, annFile=os.path.join(annotations_folder, 'captions_val2017.json'), transform=None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NMI1aStph0CM"
   },
   "source": [
    "# Processor and Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "collapsed": true,
    "executionInfo": {
     "elapsed": 2089,
     "status": "ok",
     "timestamp": 1740516367734,
     "user": {
      "displayName": "Zihao Ye",
      "userId": "17726494799981155241"
     },
     "user_tz": 300
    },
    "id": "1Az0ep2Lhytm",
    "outputId": "2c998ace-7f0a-47c3-8e31-155735f34f3e"
   },
   "outputs": [],
   "source": [
    "processor = AutoProcessor.from_pretrained('openai/clip-vit-base-patch32')\n",
    "tokenizer = open_clip.get_tokenizer('ViT-B-32')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CKCofCzQgH-E"
   },
   "source": [
    "# Encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "collapsed": true,
    "executionInfo": {
     "elapsed": 84797,
     "status": "ok",
     "timestamp": 1740516999917,
     "user": {
      "displayName": "Zihao Ye",
      "userId": "17726494799981155241"
     },
     "user_tz": 300
    },
    "id": "2XjXyKLJVvBU",
    "outputId": "a5d95616-b223-4eff-fece-96b30a38c702"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cloning into 'data/CC12M_TripletCLIP_ViTB12'...\n",
      "remote: Enumerating objects: 12, done.\u001b[K\n",
      "remote: Counting objects: 100% (9/9), done.\u001b[K\n",
      "remote: Compressing objects: 100% (9/9), done.\u001b[K\n",
      "remote: Total 12 (delta 1), reused 0 (delta 0), pack-reused 3 (from 1)\u001b[K\n",
      "Unpacking objects: 100% (12/12), 5.43 KiB | 1.36 MiB/s, done.\n",
      "Filtering content: 100% (2/2), 577.12 MiB | 16.46 MiB/s, done.\n",
      "Updated Git hooks.\n",
      "Git LFS initialized.\n",
      "Cloning into 'data/CC12M_NegCLIP_ViTB12'...\n",
      "remote: Enumerating objects: 12, done.\u001b[K\n",
      "remote: Counting objects: 100% (9/9), done.\u001b[K\n",
      "remote: Compressing objects: 100% (9/9), done.\u001b[K\n",
      "remote: Total 12 (delta 1), reused 0 (delta 0), pack-reused 3 (from 1)\u001b[K\n",
      "Unpacking objects: 100% (12/12), 5.43 KiB | 1.09 MiB/s, done.\n",
      "Filtering content: 100% (2/2), 577.12 MiB | 16.35 MiB/s, done.\n",
      "Updated Git hooks.\n",
      "Git LFS initialized.\n",
      "Cloning into 'data/CC12M_LaCLIP_ViTB12'...\n",
      "remote: Enumerating objects: 12, done.\u001b[K\n",
      "remote: Counting objects: 100% (9/9), done.\u001b[K\n",
      "remote: Compressing objects: 100% (9/9), done.\u001b[K\n",
      "remote: Total 12 (delta 1), reused 0 (delta 0), pack-reused 3 (from 1)\u001b[K\n",
      "Unpacking objects: 100% (12/12), 5.43 KiB | 1.08 MiB/s, done.\n",
      "Filtering content: 100% (2/2), 577.12 MiB | 16.38 MiB/s, done.\n",
      "Updated Git hooks.\n",
      "Git LFS initialized.\n",
      "Cloning into 'data/CC12M_NegCLIPPP_ViTB12'...\n",
      "remote: Enumerating objects: 12, done.\u001b[K\n",
      "remote: Counting objects: 100% (9/9), done.\u001b[K\n",
      "remote: Compressing objects: 100% (9/9), done.\u001b[K\n",
      "remote: Total 12 (delta 1), reused 0 (delta 0), pack-reused 3 (from 1)\u001b[K\n",
      "Unpacking objects: 100% (12/12), 5.43 KiB | 1.09 MiB/s, done.\n",
      "Filtering content: 100% (2/2), 577.12 MiB | 16.38 MiB/s, done.\n",
      "Updated Git hooks.\n",
      "Git LFS initialized.\n"
     ]
    }
   ],
   "source": [
    "text_encoder_t, image_encoder_t = load_triplet_clip_encoders(repo_name='TripletCLIP/CC12M_TripletCLIP_ViTB12',text_path='text-encoder',image_path='vision-encoder', work_dir=data_dir, device=device)\n",
    "text_encoder_n, image_encoder_n = load_triplet_clip_encoders(repo_name='TripletCLIP/CC12M_NegCLIP_ViTB12',text_path='text-encoder',image_path='vision-encoder', work_dir=data_dir, device=device)\n",
    "# text_encoder_clip_base, image_encoder_clip_base = load_triplet_clip_encoders(repo_name='openai/clip-vit-base-patch32', work_dir='/content', device=device)\n",
    "# text_encoder_clip_large, image_encoder_clip_large = load_triplet_clip_encoders(repo_name='openai/clip-vit-large-patch14', work_dir='/content', device=device)\n",
    "text_encoder_l, image_encoder_l = load_triplet_clip_encoders(repo_name='TripletCLIP/CC12M_LaCLIP_ViTB12',text_path='text-encoder',image_path='vision-encoder', work_dir=data_dir, device=device)\n",
    "text_encoder_npp, image_encoder_npp = load_triplet_clip_encoders(repo_name='TripletCLIP/CC12M_NegCLIPPP_ViTB12',text_path='text-encoder',image_path='vision-encoder', work_dir=data_dir, device=device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "57-NWhxck_SI"
   },
   "source": [
    "## Feature Extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 435
    },
    "executionInfo": {
     "elapsed": 400993,
     "status": "error",
     "timestamp": 1740516877874,
     "user": {
      "displayName": "Zihao Ye",
      "userId": "17726494799981155241"
     },
     "user_tz": 300
    },
    "id": "qVbzAZD0eKLH",
    "outputId": "c932e67f-f452-4b46-b5d1-771b1c164cf4"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 5000/5000 [01:24<00:00, 58.92it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([5000, 512])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 5000/5000 [01:25<00:00, 58.42it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([5000, 512])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 5000/5000 [01:24<00:00, 59.12it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([5000, 512])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 5000/5000 [01:26<00:00, 58.10it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([5000, 512])\n"
     ]
    }
   ],
   "source": [
    "sample_size = 5000\n",
    "image_features_t, text_features_t = transform(coco, image_encoder_t, text_encoder_t, processor, tokenizer, device, sample_size=sample_size)\n",
    "image_features_n, text_features_n = transform(coco, image_encoder_n, text_encoder_n, processor, tokenizer, device, sample_size=sample_size)\n",
    "# image_features_clip_base, text_features_clip_base = transform(coco, image_encoder_clip_base, text_encoder_clip_base, processor, tokenizer, device, sample_size=sample_size)\n",
    "image_features_l, text_features_l = transform(coco, image_encoder_l, text_encoder_l, processor, tokenizer, device, sample_size=sample_size)\n",
    "image_features_npp, text_features_npp = transform(coco, image_encoder_npp, text_encoder_npp, processor, tokenizer, device, sample_size=sample_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GVTKdPeOvRSp"
   },
   "source": [
    "# Similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "executionInfo": {
     "elapsed": 8,
     "status": "ok",
     "timestamp": 1740514500291,
     "user": {
      "displayName": "Zihao Ye",
      "userId": "17726494799981155241"
     },
     "user_tz": 300
    },
    "id": "iTSSQnHujCo_"
   },
   "outputs": [],
   "source": [
    "def rotate_and_calc_similarity(image_features, text_features):\n",
    "\n",
    "    # ========== 1) 计算全局均值并归一化 ==========\n",
    "    # image 全局均值: (D,)\n",
    "    mean_image = image_features.mean(dim=0)\n",
    "    # text 全局均值:  (D,)\n",
    "    mean_text  = text_features.mean(dim=0)\n",
    "\n",
    "    # 归一化 (若想直接点乘当作余弦相似度, image/text_features 本身也需归一化)\n",
    "    mean_image_norm = mean_image / (mean_image.norm() + 1e-12)\n",
    "    mean_text_norm  = mean_text  / (mean_text.norm()  + 1e-12)\n",
    "\n",
    "    # ========== 2) 计算夹角 theta 并构造二维平面内的正交向量 ==========\n",
    "    # cos_angle = a·b\n",
    "    cos_angle = torch.dot(mean_image_norm, mean_text_norm)\n",
    "    # 防止浮点误差导致 acos 输入超出 [-1,1]\n",
    "    cos_angle = torch.clamp(cos_angle, -1.0, 1.0)\n",
    "    theta = torch.acos(cos_angle)  # 弧度\n",
    "\n",
    "    sin_angle = torch.sqrt(1 - cos_angle**2 + 1e-12)\n",
    "    # 在 (mean_image_norm, mean_text_norm) 所张平面上，构造与 mean_image_norm 正交的单位向量 v\n",
    "    v = (mean_text_norm - cos_angle * mean_image_norm) / (sin_angle + 1e-12)\n",
    "\n",
    "    # ========== 3) 在 (mean_image_norm, v) 平面内对 image_features 做旋转 ==========\n",
    "    # 投影系数\n",
    "    proj_a = image_features @ mean_image_norm  # (N,)\n",
    "    proj_v = image_features @ v                # (N,)\n",
    "\n",
    "    # 2D 旋转\n",
    "    # new_a = a*cosθ - v*sinθ\n",
    "    # new_v = a*sinθ + v*cosθ\n",
    "    rotated_proj_a = proj_a * torch.cos(theta) - proj_v * torch.sin(theta)  # (N,)\n",
    "    rotated_proj_v = proj_a * torch.sin(theta) + proj_v * torch.cos(theta)  # (N,)\n",
    "\n",
    "    # 在平面内的分量(旋转后)\n",
    "    rotated_parallel = (rotated_proj_a.unsqueeze(1) * mean_image_norm.unsqueeze(0)\n",
    "                      + rotated_proj_v.unsqueeze(1) * v.unsqueeze(0))\n",
    "\n",
    "    # 原本在平面内的分量\n",
    "    orig_parallel = (proj_a.unsqueeze(1) * mean_image_norm.unsqueeze(0)\n",
    "                   + proj_v.unsqueeze(1) * v.unsqueeze(0))\n",
    "\n",
    "    # 正交分量(不在平面内, 保持不变)\n",
    "    orthogonal_component = image_features - orig_parallel\n",
    "\n",
    "    # 旋转后的图像特征 (N, D)\n",
    "    rotated_image_features = rotated_parallel + orthogonal_component\n",
    "\n",
    "    # ========== 4) 计算相似度矩阵 (N, 5N) ==========\n",
    "    # 如果想用余弦相似度, 这里要保证 rotated_image_features 与 text_features 已各自归一化。\n",
    "    # 否则就是一般点乘。\n",
    "    similarity_matrix = rotated_image_features @ text_features.T  # (N, 5N)\n",
    "\n",
    "    # ========== 5) 通过广播为 \"正例\" 与 \"负例\" 构造布尔掩码 ==========\n",
    "    # 对第 i 行(图像 i), 正例对应的列区间是 [5*i, ..., 5*i+4]\n",
    "    #   => j_idx // 5 == i_idx\n",
    "\n",
    "    N = image_features.size(0)\n",
    "    i_idx = torch.arange(N, device=similarity_matrix.device).unsqueeze(1).expand(N, 5*N)\n",
    "    j_idx = torch.arange(5*N, device=similarity_matrix.device).unsqueeze(0).expand(N, 5*N)\n",
    "\n",
    "    pos_mask = (j_idx // 5 == i_idx)  # 形状 (N, 5N), True 表示匹配\n",
    "    # 所有正例相似度 => (N*5,) 向量\n",
    "    pos_sims = similarity_matrix[pos_mask]\n",
    "    # 如果你想分别取 \"每个图像 vs. 它的 5 条文本\" 的平均，可以 reshape => (N,5) 再 mean\n",
    "    # 这会先对每张图像5个caption做平均，再对 N 张图像平均\n",
    "    mean_pos = pos_sims.view(N, 5).mean()\n",
    "\n",
    "    # 负例相似度(不匹配)\n",
    "    neg_sims = similarity_matrix[~pos_mask]  # (N*(5N-5),)\n",
    "    mean_neg = neg_sims.mean()\n",
    "\n",
    "    return mean_pos.item(), mean_neg.item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 116,
     "status": "ok",
     "timestamp": 1740514502376,
     "user": {
      "displayName": "Zihao Ye",
      "userId": "17726494799981155241"
     },
     "user_tz": 300
    },
    "id": "dANQqSrlvR6J",
    "outputId": "b54944f5-9995-42d9-eeca-de3d09d00d3b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TripletCLIP Similarity    - Positive: 0.7865129709243774, Negative: 0.6824867129325867, Pos-Neg: 0.10402625799179077\n",
      "NegativeCLIP Similarity   - Positive: 0.6373680830001831, Negative: 0.47875773906707764, Pos-Neg: 0.15861034393310547\n",
      "LaCLIP Similarity         - Positive: 0.5077755451202393, Negative: 0.3667873442173004, Pos-Neg: 0.14098820090293884\n",
      "NegativeCLIP++ Similarity - Positive: 0.6485074162483215, Negative: 0.49330487847328186, Pos-Neg: 0.15520253777503967\n"
     ]
    }
   ],
   "source": [
    "sim_pos_t, sim_neg_t = rotate_and_calc_similarity(image_features_t, text_features_t)\n",
    "sim_pos_n, sim_neg_n = rotate_and_calc_similarity(image_features_n, text_features_n)\n",
    "sim_pos_l, sim_neg_l = rotate_and_calc_similarity(image_features_l, text_features_l)\n",
    "sim_pos_npp, sim_neg_npp = rotate_and_calc_similarity(image_features_npp, text_features_npp)\n",
    "print(f'TripletCLIP Similarity    - Positive: {sim_pos_t}, Negative: {sim_neg_t}, Pos-Neg: {sim_pos_t - sim_neg_t}')\n",
    "print(f'NegativeCLIP Similarity   - Positive: {sim_pos_n}, Negative: {sim_neg_n}, Pos-Neg: {sim_pos_n - sim_neg_n}')\n",
    "print(f'LaCLIP Similarity         - Positive: {sim_pos_l}, Negative: {sim_neg_l}, Pos-Neg: {sim_pos_l - sim_neg_l}')\n",
    "print(f'NegativeCLIP++ Similarity - Positive: {sim_pos_npp}, Negative: {sim_neg_npp}, Pos-Neg: {sim_pos_npp - sim_neg_npp}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Retrieval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "id": "lhKD95zLsWWq"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "def evaluate_retrieval(image_features, text_features, num_captions=5):\n",
    "    \"\"\"\n",
    "    评估 image-to-text (I2T) 和 text-to-image (T2I) 的检索性能指标。\n",
    "    \n",
    "    Args:\n",
    "        image_features (torch.Tensor): shape 为 (N, D) 的图片特征张量，其中 N 为图片数量。\n",
    "        text_features (torch.Tensor): shape 为 (N * num_captions, D) 的文本特征张量，每张图片对应 num_captions 个 caption。\n",
    "        num_captions (int): 每张图片对应的 caption 数量（默认 5）。\n",
    "    \n",
    "    Returns:\n",
    "        dict: 包含以下键值对的字典：\n",
    "            {\n",
    "                'I2T_top1': float,  # 图片检索文本 Top1 准确率\n",
    "                'I2T_top5': float,  # 图片检索文本 Top5 准确率\n",
    "                'I2T_top10': float, # 图片检索文本 Top10 准确率\n",
    "                'T2I_top1': float,  # 文本检索图片 Top1 准确率\n",
    "                'T2I_top5': float,  # 文本检索图片 Top5 准确率\n",
    "                'T2I_top10': float, # 文本检索图片 Top10 准确率\n",
    "            }\n",
    "    \"\"\"\n",
    "    # 确保文本数量与图片数量及每张图片的 caption 数匹配\n",
    "    num_images = image_features.size(0)\n",
    "    assert text_features.size(0) == num_images * num_captions, \"文本特征数量与图片数量不匹配！\"\n",
    "\n",
    "    # 若未归一化，则先归一化特征（如果已归一化可省略）\n",
    "    image_features = image_features / image_features.norm(dim=-1, keepdim=True)\n",
    "    text_features = text_features / text_features.norm(dim=-1, keepdim=True)\n",
    "    \n",
    "    # 计算相似度矩阵：shape 为 (N, N * num_captions)\n",
    "    similarity = image_features @ text_features.t()\n",
    "    \n",
    "    # 评估 image-to-text (I2T)\n",
    "    I2T_top1, I2T_top5, I2T_top10 = 0, 0, 0\n",
    "    for i in range(num_images):\n",
    "        sim_i = similarity[i]  # 第 i 张图片与所有文本之间的相似度\n",
    "        # 获取从大到小排序后的文本索引\n",
    "        sorted_indices = torch.argsort(sim_i, descending=True)\n",
    "        # 第 i 张图片的 ground truth 文本索引范围\n",
    "        gt_indices = list(range(i * num_captions, i * num_captions + num_captions))\n",
    "        # Top1 检索：如果 ground truth 中任一索引出现在前 1 个，则认为正确\n",
    "        if any(idx in sorted_indices[:1] for idx in gt_indices):\n",
    "            I2T_top1 += 1\n",
    "        # Top5 检索\n",
    "        if any(idx in sorted_indices[:5] for idx in gt_indices):\n",
    "            I2T_top5 += 1\n",
    "        # Top10 检索\n",
    "        if any(idx in sorted_indices[:10] for idx in gt_indices):\n",
    "            I2T_top10 += 1\n",
    "\n",
    "    I2T_top1_score = I2T_top1 / num_images\n",
    "    I2T_top5_score = I2T_top5 / num_images\n",
    "    I2T_top10_score = I2T_top10 / num_images\n",
    "\n",
    "    # 评估 text-to-image (T2I)\n",
    "    # 这里可以利用相似度矩阵的转置，shape 为 (N * num_captions, N)\n",
    "    similarity_t = similarity.t()\n",
    "    T2I_top1, T2I_top5, T2I_top10 = 0, 0, 0\n",
    "    for j in range(text_features.size(0)):\n",
    "        sim_j = similarity_t[j]  # 第 j 个文本与所有图片的相似度\n",
    "        sorted_indices = torch.argsort(sim_j, descending=True)\n",
    "        # 对于第 j 个文本，其对应图片索引为 j // num_captions\n",
    "        gt_image = j // num_captions\n",
    "        if gt_image in sorted_indices[:1]:\n",
    "            T2I_top1 += 1\n",
    "        if gt_image in sorted_indices[:5]:\n",
    "            T2I_top5 += 1\n",
    "        if gt_image in sorted_indices[:10]:\n",
    "            T2I_top10 += 1\n",
    "\n",
    "    total_texts = text_features.size(0)\n",
    "    T2I_top1_score = T2I_top1 / total_texts\n",
    "    T2I_top5_score = T2I_top5 / total_texts\n",
    "    T2I_top10_score = T2I_top10 / total_texts\n",
    "\n",
    "    results = {\n",
    "        'I2T_top1': I2T_top1_score,\n",
    "        'I2T_top5': I2T_top5_score,\n",
    "        'I2T_top10': I2T_top10_score,\n",
    "        'T2I_top1': T2I_top1_score,\n",
    "        'T2I_top5': T2I_top5_score,\n",
    "        'T2I_top10': T2I_top10_score,\n",
    "    }\n",
    "\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image to Text (I2T) Top1  - TripletCLIP: 0.1416, NegCLIP: 0.1224, LaCLIP: 0.0982, NegCLIP++: 0.1098\n",
      "Image to Text (I2T) Top5  - TripletCLIP: 0.3358, NegCLIP: 0.293, LaCLIP: 0.2552, NegCLIP++: 0.2756\n",
      "Image to Text (I2T) Top10 - TripletCLIP: 0.4494, NegCLIP: 0.4014, LaCLIP: 0.3526, NegCLIP++: 0.3736\n",
      "Text to Image (T2I) Top1  - TripletCLIP: 0.11348, NegCLIP: 0.08208, LaCLIP: 0.07116, NegCLIP++: 0.0836\n",
      "Text to Image (T2I) Top5  - TripletCLIP: 0.27908, NegCLIP: 0.22212, LaCLIP: 0.1922, NegCLIP++: 0.2232\n",
      "Text to Image (T2I) Top10 - TripletCLIP: 0.38332, NegCLIP: 0.31744, LaCLIP: 0.27956, NegCLIP++: 0.3184\n"
     ]
    }
   ],
   "source": [
    "results_t = evaluate_retrieval(image_features_t, text_features_t)\n",
    "results_n = evaluate_retrieval(image_features_n, text_features_n)\n",
    "results_l = evaluate_retrieval(image_features_l, text_features_l)\n",
    "results_npp = evaluate_retrieval(image_features_npp, text_features_npp)\n",
    "print(f'Image to Text (I2T) Top1  - TripletCLIP: {results_t[\"I2T_top1\"]}, NegCLIP: {results_n[\"I2T_top1\"]}, LaCLIP: {results_l[\"I2T_top1\"]}, NegCLIP++: {results_npp[\"I2T_top1\"]}')\n",
    "print(f'Image to Text (I2T) Top5  - TripletCLIP: {results_t[\"I2T_top5\"]}, NegCLIP: {results_n[\"I2T_top5\"]}, LaCLIP: {results_l[\"I2T_top5\"]}, NegCLIP++: {results_npp[\"I2T_top5\"]}')\n",
    "print(f'Image to Text (I2T) Top10 - TripletCLIP: {results_t[\"I2T_top10\"]}, NegCLIP: {results_n[\"I2T_top10\"]}, LaCLIP: {results_l[\"I2T_top10\"]}, NegCLIP++: {results_npp[\"I2T_top10\"]}')\n",
    "print(f'Text to Image (T2I) Top1  - TripletCLIP: {results_t[\"T2I_top1\"]}, NegCLIP: {results_n[\"T2I_top1\"]}, LaCLIP: {results_l[\"T2I_top1\"]}, NegCLIP++: {results_npp[\"T2I_top1\"]}')\n",
    "print(f'Text to Image (T2I) Top5  - TripletCLIP: {results_t[\"T2I_top5\"]}, NegCLIP: {results_n[\"T2I_top5\"]}, LaCLIP: {results_l[\"T2I_top5\"]}, NegCLIP++: {results_npp[\"T2I_top5\"]}')\n",
    "print(f'Text to Image (T2I) Top10 - TripletCLIP: {results_t[\"T2I_top10\"]}, NegCLIP: {results_n[\"T2I_top10\"]}, LaCLIP: {results_l[\"T2I_top10\"]}, NegCLIP++: {results_npp[\"T2I_top10\"]}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Uniformity and Alignment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "# alignment metric\n",
    "# features: (N, D)\n",
    "import numpy as np\n",
    "def compute_alignment(text_features, image_features, alpha=2, captions_per_image=5):\n",
    "    # image_features: shape [N, D]\n",
    "    # text_features: shape [N * captions_per_image, D]\n",
    "    num_images, D = image_features.shape\n",
    "    # 重塑为 [N, captions_per_image, D]\n",
    "    text_features = text_features.reshape(num_images, captions_per_image, D)\n",
    "    # 扩展 image_features 为 [N, 1, D] 以便广播\n",
    "    image_features_expanded = image_features[:, None, :]\n",
    "    # 计算每个 caption 与对应 image 的差异\n",
    "    diff = text_features - image_features_expanded  # shape: [N, captions_per_image, D]\n",
    "    # L2 范数，计算每个 caption 的距离\n",
    "    dist = np.linalg.norm(diff, axis=2)  # shape: [N, captions_per_image]\n",
    "    # 对所有距离的 alpha 次方取均值\n",
    "    return np.mean(dist ** alpha)\n",
    "\n",
    "# uniformity metric\n",
    "def compute_uniformity_approx(features, t=2.0, num_samples=100000):\n",
    "    \"\"\"\n",
    "    Approximate uniformity via random sampling of pairs.\n",
    "    \n",
    "    L = log( E_{x,y}[ exp(-t ||x-y||^2) ] )\n",
    "    \"\"\"\n",
    "    N = features.shape[0]\n",
    "    # Randomly sample pairs (i, j)\n",
    "    idx1 = np.random.randint(0, N, size=num_samples)\n",
    "    idx2 = np.random.randint(0, N, size=num_samples)\n",
    "    \n",
    "    # Compute squared distances for the sampled pairs\n",
    "    diff = features[idx1] - features[idx2]\n",
    "    dist2 = np.sum(diff * diff, axis=-1)\n",
    "    \n",
    "    # Compute exp(-t * dist^2) and take the average\n",
    "    values = np.exp(-t * dist2)\n",
    "    avg = np.mean(values)\n",
    "    \n",
    "    return np.log(avg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TripletCLIP - Alignment: 1.3321770429611206, Uniformity Text: -1.4073137044906616, Uniformity Image: -1.0404778718948364\n",
      "NegCLIP     - Alignment: 1.3323582410812378, Uniformity Text: -1.21366286277771, Uniformity Image: -2.557321071624756\n",
      "LaCLIP      - Alignment: 1.3822613954544067, Uniformity Text: -1.5379735231399536, Uniformity Image: -3.0081543922424316\n",
      "NegCLIP++   - Alignment: 1.3408807516098022, Uniformity Text: -1.219635248184204, Uniformity Image: -2.4809818267822266\n"
     ]
    }
   ],
   "source": [
    "alignment_t = compute_alignment(text_features_t.cpu().numpy(), image_features_t.cpu().numpy())\n",
    "alignment_n = compute_alignment(text_features_n.cpu().numpy(), image_features_n.cpu().numpy())\n",
    "alignment_l = compute_alignment(text_features_l.cpu().numpy(), image_features_l.cpu().numpy())\n",
    "alignment_npp = compute_alignment(text_features_npp.cpu().numpy(), image_features_npp.cpu().numpy())\n",
    "uniformity_text_t = compute_uniformity_approx(text_features_t.cpu().numpy())\n",
    "uniformity_image_t = compute_uniformity_approx(image_features_t.cpu().numpy())\n",
    "uniformity_text_n = compute_uniformity_approx(text_features_n.cpu().numpy())\n",
    "uniformity_image_n = compute_uniformity_approx(image_features_n.cpu().numpy())\n",
    "uniformity_text_l = compute_uniformity_approx(text_features_l.cpu().numpy())\n",
    "uniformity_image_l = compute_uniformity_approx(image_features_l.cpu().numpy())\n",
    "uniformity_text_npp = compute_uniformity_approx(text_features_npp.cpu().numpy())\n",
    "uniformity_image_npp = compute_uniformity_approx(image_features_npp.cpu().numpy())\n",
    "print(\"TripletCLIP - Alignment: {}, Uniformity Text: {}, Uniformity Image: {}\".format(alignment_t, uniformity_text_t, uniformity_image_t))\n",
    "print(\"NegCLIP     - Alignment: {}, Uniformity Text: {}, Uniformity Image: {}\".format(alignment_n, uniformity_text_n, uniformity_image_n))\n",
    "print(\"LaCLIP      - Alignment: {}, Uniformity Text: {}, Uniformity Image: {}\".format(alignment_l, uniformity_text_l, uniformity_image_l))\n",
    "print(\"NegCLIP++   - Alignment: {}, Uniformity Text: {}, Uniformity Image: {}\".format(alignment_npp, uniformity_text_npp, uniformity_image_npp))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CKA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def linear_cka(X, Y):\n",
    "    \"\"\"\n",
    "    计算线性 CKA (PyTorch 版本)。\n",
    "    X, Y 都是 torch.Tensor，形状 (n, d)。\n",
    "    \"\"\"\n",
    "    # X^T Y\n",
    "    XY = X.T @ Y  # (d_X, d_Y)\n",
    "\n",
    "    # 分子\n",
    "    numerator = torch.sum(XY**2)\n",
    "\n",
    "    # 分母\n",
    "    XX = X.T @ X\n",
    "    YY = Y.T @ Y\n",
    "    denominator = torch.sqrt(torch.sum(XX**2) * torch.sum(YY**2))\n",
    "\n",
    "    return numerator / denominator\n",
    "\n",
    "def replicate_features(X: torch.Tensor, replicate_times: int = 5) -> torch.Tensor:\n",
    "    \"\"\"\n",
    "    将形状为 (N, d) 的特征张量 X 中的每个样本复制 replicate_times 次，\n",
    "    返回形状为 (N * replicate_times, d)。\n",
    "    \"\"\"\n",
    "    # X.shape = (N, d)\n",
    "    N, d = X.shape\n",
    "\n",
    "    X_extended = X.unsqueeze(1).expand(N, replicate_times, d).reshape(-1, d)\n",
    "\n",
    "    return X_extended"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TripletCLIP CKA: 0.9902653098106384\n",
      "NegCLIP     CKA: 0.9574718475341797\n",
      "LaCLIP      CKA: 0.9180614948272705\n",
      "NegCLIP++   CKA: 0.963750958442688\n"
     ]
    }
   ],
   "source": [
    "cka_t = linear_cka(text_features_t, replicate_features(image_features_t, 5))\n",
    "cka_n = linear_cka(text_features_n, replicate_features(image_features_n, 5))\n",
    "cka_l = linear_cka(text_features_l, replicate_features(image_features_l, 5))\n",
    "cka_npp = linear_cka(text_features_npp, replicate_features(image_features_npp, 5))\n",
    "print(\"TripletCLIP CKA: {}\".format(cka_t))\n",
    "print(\"NegCLIP     CKA: {}\".format(cka_n))\n",
    "print(\"LaCLIP      CKA: {}\".format(cka_l))\n",
    "print(\"NegCLIP++   CKA: {}\".format(cka_npp))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Text Features CKA:\n",
      "            t         n         l       npp\n",
      "t    1.000000  0.994721  0.992668  0.995431\n",
      "n    0.994721  1.000000  0.993695  0.996771\n",
      "l    0.992668  0.993695  1.000000  0.994085\n",
      "npp  0.995431  0.996771  0.994085  1.000000\n",
      "\n",
      "Image Features CKA:\n",
      "            t         n         l       npp\n",
      "t    1.000000  0.966823  0.930975  0.972535\n",
      "n    0.966823  1.000000  0.954775  0.978458\n",
      "l    0.930975  0.954775  1.000000  0.954309\n",
      "npp  0.972535  0.978458  0.954309  1.000000\n"
     ]
    }
   ],
   "source": [
    "cka_text_t_n = linear_cka(text_features_t, text_features_n)\n",
    "cka_text_t_l = linear_cka(text_features_t, text_features_l)\n",
    "cka_text_t_npp = linear_cka(text_features_t, text_features_npp)\n",
    "cka_text_n_l = linear_cka(text_features_n, text_features_l)\n",
    "cka_text_n_npp = linear_cka(text_features_n, text_features_npp)\n",
    "cka_text_l_npp = linear_cka(text_features_l, text_features_npp)\n",
    "cka_image_t_n = linear_cka(image_features_t, image_features_n)\n",
    "cka_image_t_l = linear_cka(image_features_t, image_features_l)\n",
    "cka_image_t_npp = linear_cka(image_features_t, image_features_npp)\n",
    "cka_image_n_l = linear_cka(image_features_n, image_features_l)\n",
    "cka_image_n_npp = linear_cka(image_features_n, image_features_npp)\n",
    "cka_image_l_npp = linear_cka(image_features_l, image_features_npp)\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "labels = [\"t\", \"n\", \"l\", \"npp\"]\n",
    "\n",
    "# 构造文本特征的表格数据（转换为标量）\n",
    "text_data = [\n",
    "    [1,                   cka_text_t_n.item(),   cka_text_t_l.item(),    cka_text_t_npp.item()],\n",
    "    [cka_text_t_n.item(), 1,                     cka_text_n_l.item(),    cka_text_n_npp.item()],\n",
    "    [cka_text_t_l.item(), cka_text_n_l.item(),   1,                      cka_text_l_npp.item()],\n",
    "    [cka_text_t_npp.item(), cka_text_n_npp.item(), cka_text_l_npp.item(),  1]\n",
    "]\n",
    "\n",
    "# 构造图像特征的表格数据\n",
    "image_data = [\n",
    "    [1,                    cka_image_t_n.item(),   cka_image_t_l.item(),    cka_image_t_npp.item()],\n",
    "    [cka_image_t_n.item(), 1,                      cka_image_n_l.item(),    cka_image_n_npp.item()],\n",
    "    [cka_image_t_l.item(), cka_image_n_l.item(),   1,                      cka_image_l_npp.item()],\n",
    "    [cka_image_t_npp.item(), cka_image_n_npp.item(), cka_image_l_npp.item(),  1]\n",
    "]\n",
    "\n",
    "# 使用 DataFrame 显示\n",
    "df_text = pd.DataFrame(text_data, index=labels, columns=labels)\n",
    "df_image = pd.DataFrame(image_data, index=labels, columns=labels)\n",
    "\n",
    "print(\"Text Features CKA:\")\n",
    "print(df_text)\n",
    "print(\"\\nImage Features CKA:\")\n",
    "print(df_image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CKA between text and image features:\n",
      "            t         n         l       npp\n",
      "t    0.990265  0.990506  0.985396  0.991323\n",
      "n    0.958043  0.957472  0.953378  0.958353\n",
      "l    0.922640  0.921082  0.918061  0.921956\n",
      "npp  0.963544  0.962884  0.958622  0.963751\n"
     ]
    }
   ],
   "source": [
    "cka_t_img_n_text = linear_cka(replicate_features(image_features_t, 5), text_features_n)\n",
    "cka_t_img_l_text = linear_cka(replicate_features(image_features_t, 5), text_features_l)\n",
    "cka_t_img_npp_text = linear_cka(replicate_features(image_features_t, 5), text_features_npp)\n",
    "cka_n_img_t_text = linear_cka(replicate_features(image_features_n, 5), text_features_t)\n",
    "cka_n_img_l_text = linear_cka(replicate_features(image_features_n, 5), text_features_l)\n",
    "cka_n_img_npp_text = linear_cka(replicate_features(image_features_n, 5), text_features_npp)\n",
    "cka_l_img_t_text = linear_cka(replicate_features(image_features_l, 5), text_features_t)\n",
    "cka_l_img_n_text = linear_cka(replicate_features(image_features_l, 5), text_features_n)\n",
    "cka_l_img_npp_text = linear_cka(replicate_features(image_features_l, 5), text_features_npp)\n",
    "cka_npp_img_t_text = linear_cka(replicate_features(image_features_npp, 5), text_features_t)\n",
    "cka_npp_img_n_text = linear_cka(replicate_features(image_features_npp, 5), text_features_n)\n",
    "cka_npp_img_l_text = linear_cka(replicate_features(image_features_npp, 5), text_features_l)\n",
    "\n",
    "labels = [\"t\", \"n\", \"l\", \"npp\"]\n",
    "# 构造文本特征的表格数据（转换为标量）\n",
    "data = [\n",
    "    [cka_t.item(), cka_t_img_n_text.item(), cka_t_img_l_text.item(), cka_t_img_npp_text.item()],\n",
    "    [cka_n_img_t_text.item(), cka_n.item(), cka_n_img_l_text.item(), cka_n_img_npp_text.item()],\n",
    "    [cka_l_img_t_text.item(), cka_l_img_n_text.item(), cka_l.item(), cka_l_img_npp_text.item()],\n",
    "    [cka_npp_img_t_text.item(), cka_npp_img_n_text.item(), cka_npp_img_l_text.item(), cka_npp.item()]\n",
    "]\n",
    "\n",
    "df = pd.DataFrame(data, index=labels, columns=labels)\n",
    "print(\"CKA between text and image features:\")\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": [
    {
     "file_id": "1Y9euq1P6zWnVoZHztyFuI1MDdh_vF06r",
     "timestamp": 1740456280352
    },
    {
     "file_id": "1ouBrZT4RCTzUlIq7s4UtcG6d4wQqft5w",
     "timestamp": 1740449638145
    }
   ]
  },
  "kernelspec": {
   "display_name": "time_series",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
