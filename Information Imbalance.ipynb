{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Initialization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.distributions as D\n",
    "import time\n",
    "import random\n",
    "import numpy as np\n",
    "\n",
    "# seed = 1234\n",
    "# torch.manual_seed(seed)\n",
    "# random.seed(seed)\n",
    "# np.random.seed(seed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Mixture Gaussians"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mix_gaussians(components_num, dim_num):\n",
    "    # random.weights\n",
    "    raw_weights = torch.rand(components_num)\n",
    "    weights = raw_weights / raw_weights.sum()\n",
    "\n",
    "    # random.means\n",
    "    means = torch.randn(components_num, dim_num)\n",
    "\n",
    "    # random.covs\n",
    "    covs = []\n",
    "    epsilon = 1e-3\n",
    "    for _ in range(components_num):\n",
    "        A = torch.randn(dim_num, dim_num)\n",
    "        cov = A @ A.t() + epsilon * torch.eye(dim_num)\n",
    "        covs.append(cov)\n",
    "\n",
    "    # create components\n",
    "    components = []\n",
    "    for i in range(components_num):\n",
    "        dist = D.MultivariateNormal(loc=means[i], covariance_matrix=covs[i])\n",
    "        components.append(dist)\n",
    "\n",
    "    return components"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generate Samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample_sequence_pairs(comp1, comp2, components_num, N, seq_len):\n",
    "    \"\"\"\n",
    "    comp1, comp2: list[MultivariateNormal]，每个长度=components_num\n",
    "    latent_indices: shape (N, seq_len)，表示每个样本在每个时刻要用哪个component\n",
    "\n",
    "    返回:\n",
    "      samples1: (N, seq_len, dim)\n",
    "      samples2: (N, seq_len, dim)\n",
    "    \"\"\"\n",
    "    latent_indices = torch.randint(0, components_num, (N, seq_len))\n",
    "    N, seq_len = latent_indices.shape\n",
    "    dim = comp1[0].event_shape[0]  # 每个component的输出维度相同\n",
    "\n",
    "    samples1 = torch.empty(N, seq_len, dim)\n",
    "    samples2 = torch.empty(N, seq_len, dim)\n",
    "\n",
    "    # 逐个样本、逐个时间步采样\n",
    "    for i in range(N):\n",
    "        for t in range(seq_len):\n",
    "            idx = latent_indices[i, t].item()\n",
    "            samples1[i, t] = comp1[idx].sample()  # shape: (dim,)\n",
    "            samples2[i, t] = comp2[idx].sample()\n",
    "\n",
    "    return samples1, samples2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "samples1 shape: torch.Size([1, 10])\n",
      "samples2 shape: torch.Size([1, 10])\n"
     ]
    }
   ],
   "source": [
    "# hyperparameters\n",
    "N = 1               # number of pairs\n",
    "components_num = 10  # number of components\n",
    "dim_num = 10         # dimension of each component\n",
    "\n",
    "# generate two mixtures of Gaussians\n",
    "cat1, comp1 = mix_gaussians(components_num, dim_num)\n",
    "cat2, comp2 = mix_gaussians(components_num, dim_num)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d_model = 64        # model dimension\n",
    "nhead = 8           # multihead attention head number\n",
    "num_layers = 2      # Transformer layer number\n",
    "dim_feedforward = 256  # feedforward dimension\n",
    "\n",
    "# Transformer encoder layer\n",
    "encoder_layer = nn.TransformerEncoderLayer(\n",
    "    d_model=d_model,\n",
    "    nhead=nhead,\n",
    "    dim_feedforward=dim_feedforward,\n",
    "    batch_first=True # (batch_size, seq_len, d_model)\n",
    ")\n",
    "\n",
    "# stack Encoder Layer\n",
    "transformer_encoder = nn.TransformerEncoder(encoder_layer, num_layers=num_layers)\n",
    "\n",
    "# input (batch_size, seq_len, input_dim)\n",
    "# 这里我们先用一个线性层将输入映射到 d_model 维度\n",
    "input_dim = 10\n",
    "projection = nn.Linear(input_dim, d_model)\n",
    "\n",
    "# 构造一个示例 batch\n",
    "batch_size = 32\n",
    "seq_len = 20\n",
    "dummy_input = torch.randn(batch_size, seq_len, input_dim)\n",
    "projected_input = projection(dummy_input)  # 映射到 (batch_size, seq_len, d_model)\n",
    "\n",
    "# 得到 Transformer 编码器的输出\n",
    "encoded_output = transformer_encoder(projected_input)  # (batch_size, seq_len, d_model)\n",
    "print(encoded_output.shape)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "time_series",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
